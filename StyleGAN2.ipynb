{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RubeRad/tcscs/blob/master/StyleGAN2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# StyleGAN2\n",
        "\n",
        "This notebook is adapted from [this notebook published by Mikael Christensen](https://t.co/hvB4OHthB5). For information on StyleGAN2, see:\n",
        "\n",
        "* Paper: https://arxiv.org/abs/1812.04948\n",
        "* Video: https://youtu.be/kSLJriaOumA\n",
        "* Code: https://github.com/NVlabs/stylegan\n",
        "* FFHQ: https://github.com/NVlabs/ffhq-dataset\n"
      ],
      "metadata": {
        "id": "c6iMA6QfJMec"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_s8h-ilzHQc"
      },
      "source": [
        "# Setup Cells\n",
        "\n",
        "There are a handful of cells which need to be run first, to get things ready.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup 1: Import tensorflow and clone StyleGAN2 code from github\n",
        "\n",
        "After running this cell, check out what GPU device Google is letting your cloud computer use (for free)! Search for it on Amazon, how much does it cost to buy one?\n",
        "\n",
        "Also, note in the Files sidebar, there is now a directory stylegan2 -- that is code pulled down from the git repository posted by NVIDIA at https://github.com/NVlabs/stylegan2.git"
      ],
      "metadata": {
        "id": "5x4eOWgNKRoO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzDuIoMcqfBT"
      },
      "outputs": [],
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "\n",
        "# Download the code\n",
        "!git clone https://github.com/NVlabs/stylegan2.git\n",
        "%cd stylegan2\n",
        "!mkdir projection\n",
        "!mkdir projection/target\n",
        "!mkdir projection/out\n",
        "!nvcc test_nvcc.cu -o test_nvcc -run\n",
        "\n",
        "print('Tensorflow version: {}'.format(tf.__version__) )\n",
        "!nvidia-smi -L\n",
        "print('GPU Identified at: {}'.format(tf.test.gpu_device_name()))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup 2: Download pretrained model and import other modules\n",
        "\n",
        "This imports a whole bunch more standard modules (StyleGAN2 is not a standard module, which is why it was cloned from github, not just imported).\n",
        "\n",
        "Then it downloads one pretrained network; that's the setup of all the nodes, layers, weights, biases, activation function choices, etc. The comments list a large number of possible model choices, by default it chooses stylegan2-ffhq-config-f-pkl, which is for 1024x1024 images of faces."
      ],
      "metadata": {
        "id": "ZjGPkGUSLIJk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwVXBFaSuoIU"
      },
      "outputs": [],
      "source": [
        "# Download the model of choice\n",
        "\n",
        "import pretrained_networks\n",
        "\n",
        "# Choose between these pretrained models - I think 'f' is the best choice:\n",
        "\n",
        "# 1024×1024 faces\n",
        "# stylegan2-ffhq-config-a.pkl\n",
        "# stylegan2-ffhq-config-b.pkl\n",
        "# stylegan2-ffhq-config-c.pkl\n",
        "# stylegan2-ffhq-config-d.pkl\n",
        "# stylegan2-ffhq-config-e.pkl\n",
        "# stylegan2-ffhq-config-f.pkl\n",
        "\n",
        "# 512×384 cars\n",
        "# stylegan2-car-config-a.pkl\n",
        "# stylegan2-car-config-b.pkl\n",
        "# stylegan2-car-config-c.pkl\n",
        "# stylegan2-car-config-d.pkl\n",
        "# stylegan2-car-config-e.pkl\n",
        "# stylegan2-car-config-f.pkl\n",
        "\n",
        "# 256x256 horses\n",
        "# stylegan2-horse-config-a.pkl\n",
        "# stylegan2-horse-config-f.pkl\n",
        "\n",
        "# 256x256 churches\n",
        "# stylegan2-church-config-a.pkl\n",
        "# stylegan2-church-config-f.pkl\n",
        "\n",
        "# 256x256 cats\n",
        "# stylegan2-cat-config-f.pkl\n",
        "# stylegan2-cat-config-a.pkl\n",
        "network_pkl = \"gdrive:networks/stylegan2-ffhq-config-f.pkl\"\n",
        "\n",
        "# If downloads fails, due to 'Google Drive download quota exceeded' you can try downloading manually from your own Google Drive account\n",
        "# network_pkl = \"/content/drive/My Drive/GAN/stylegan2-ffhq-config-f.pkl\"\n",
        "\n",
        "print('Loading networks from \"%s\"...' % network_pkl)\n",
        "_G, _D, Gs = pretrained_networks.load_networks(network_pkl)\n",
        "noise_vars = [var for name, var in Gs.components.synthesis.vars.items() if name.startswith('noise')]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import dnnlib\n",
        "import dnnlib.tflib as tflib\n",
        "import pickle\n",
        "import re\n",
        "import sys\n",
        "from io import BytesIO\n",
        "import IPython.display\n",
        "import numpy as np\n",
        "from math import ceil\n",
        "from PIL import Image, ImageDraw\n",
        "import imageio"
      ],
      "metadata": {
        "id": "ltutdy9Q9GQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup 3: Define a bunch of useful python functions\n",
        "\n",
        "These were all from the original Mikael Christensen notebook"
      ],
      "metadata": {
        "id": "zUBg1AZQLxsL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zxbhe4uLvF_a"
      },
      "outputs": [],
      "source": [
        "# Useful utility functions...\n",
        "\n",
        "# Generates a list of images, based on a list of latent vectors (Z), and a list (or a single constant) of truncation_psi's.\n",
        "def generate_images_in_w_space(dlatents, truncation_psi=0.7):\n",
        "    Gs_kwargs = dnnlib.EasyDict()\n",
        "    Gs_kwargs.output_transform = dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=True)\n",
        "    Gs_kwargs.randomize_noise = False\n",
        "    Gs_kwargs.truncation_psi = truncation_psi\n",
        "    dlatent_avg = Gs.get_var('dlatent_avg') # [component]\n",
        "\n",
        "    imgs = []\n",
        "    for row, dlatent in log_progress(enumerate(dlatents), name = \"Generating images\"):\n",
        "        #row_dlatents = (dlatent[np.newaxis] - dlatent_avg) * np.reshape(truncation_psi, [-1, 1, 1]) + dlatent_avg\n",
        "        dl = (dlatent-dlatent_avg)*truncation_psi   + dlatent_avg\n",
        "        row_images = Gs.components.synthesis.run(dlatent,  **Gs_kwargs)\n",
        "        imgs.append(PIL.Image.fromarray(row_images[0], 'RGB'))\n",
        "    return imgs       \n",
        "\n",
        "def generate_images(zs, truncation_psi):\n",
        "    Gs_kwargs = dnnlib.EasyDict()\n",
        "    Gs_kwargs.output_transform = dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=True)\n",
        "    Gs_kwargs.randomize_noise = False\n",
        "    if not isinstance(truncation_psi, list):\n",
        "        truncation_psi = [truncation_psi] * len(zs)\n",
        "        \n",
        "    imgs = []\n",
        "    for z_idx, z in log_progress(enumerate(zs), size = len(zs), name = \"Generating images\"):\n",
        "        Gs_kwargs.truncation_psi = truncation_psi[z_idx]\n",
        "        noise_rnd = np.random.RandomState(1) # fix noise\n",
        "        tflib.set_vars({var: noise_rnd.randn(*var.shape.as_list()) for var in noise_vars}) # [height, width]\n",
        "        images = Gs.run(z, None, **Gs_kwargs) # [minibatch, height, width, channel]\n",
        "        imgs.append(PIL.Image.fromarray(images[0], 'RGB'))\n",
        "    return imgs\n",
        "\n",
        "def generate_zs_from_seeds(seeds):\n",
        "    zs = []\n",
        "    for seed_idx, seed in enumerate(seeds):\n",
        "        rnd = np.random.RandomState(seed)\n",
        "        z = rnd.randn(1, *Gs.input_shape[1:]) # [minibatch, component]\n",
        "        zs.append(z)\n",
        "    return zs\n",
        "\n",
        "# Generates a list of images, based on a list of seed for latent vectors (Z), and a list (or a single constant) of truncation_psi's.\n",
        "def generate_images_from_seeds(seeds, truncation_psi):\n",
        "    return generate_images(generate_zs_from_seeds(seeds), truncation_psi)\n",
        "\n",
        "def saveImgs(imgs, location):\n",
        "  for idx, img in log_progress(enumerate(imgs), size = len(imgs), name=\"Saving images\"):\n",
        "    file = location+ str(idx) + \".png\"\n",
        "    img.save(file)\n",
        "\n",
        "def imshow(a, format='png', jpeg_fallback=True):\n",
        "  a = np.asarray(a, dtype=np.uint8)\n",
        "  str_file = BytesIO()\n",
        "  PIL.Image.fromarray(a).save(str_file, format)\n",
        "  im_data = str_file.getvalue()\n",
        "  try:\n",
        "    disp = IPython.display.display(IPython.display.Image(im_data))\n",
        "  except IOError:\n",
        "    if jpeg_fallback and format != 'jpeg':\n",
        "      print ('Warning: image was too large to display in format \"{}\"; '\n",
        "             'trying jpeg instead.').format(format)\n",
        "      return imshow(a, format='jpeg')\n",
        "    else:\n",
        "      raise\n",
        "  return disp\n",
        "\n",
        "def showarray(a, fmt='png'):\n",
        "    a = np.uint8(a)\n",
        "    f = StringIO()\n",
        "    PIL.Image.fromarray(a).save(f, fmt)\n",
        "    IPython.display.display(IPython.display.Image(data=f.getvalue()))\n",
        "\n",
        "        \n",
        "def clamp(x, minimum, maximum):\n",
        "    return max(minimum, min(x, maximum))\n",
        "    \n",
        "def drawLatent(image,latents,x,y,x2,y2, color=(255,0,0,100)):\n",
        "  buffer = PIL.Image.new('RGBA', image.size, (0,0,0,0))\n",
        "   \n",
        "  draw = ImageDraw.Draw(buffer)\n",
        "  cy = (y+y2)/2\n",
        "  draw.rectangle([x,y,x2,y2],fill=(255,255,255,180), outline=(0,0,0,180))\n",
        "  for i in range(len(latents)):\n",
        "    mx = x + (x2-x)*(float(i)/len(latents))\n",
        "    h = (y2-y)*latents[i]*0.1\n",
        "    h = clamp(h,cy-y2,y2-cy)\n",
        "    draw.line((mx,cy,mx,cy+h),fill=color)\n",
        "  return PIL.Image.alpha_composite(image,buffer)\n",
        "             \n",
        "  \n",
        "def createImageGrid(images, scale=0.25, rows=1):\n",
        "   w,h = images[0].size\n",
        "   w = int(w*scale)\n",
        "   h = int(h*scale)\n",
        "   height = rows*h\n",
        "   cols = ceil(len(images) / rows)\n",
        "   width = cols*w\n",
        "   canvas = PIL.Image.new('RGBA', (width,height), 'white')\n",
        "   for i,img in enumerate(images):\n",
        "     img = img.resize((w,h), PIL.Image.ANTIALIAS)\n",
        "     canvas.paste(img, (w*(i % cols), h*(i // cols))) \n",
        "   return canvas\n",
        "\n",
        "def convertZtoW(latent, truncation_psi=0.7, truncation_cutoff=9):\n",
        "  dlatent = Gs.components.mapping.run(latent, None) # [seed, layer, component]\n",
        "  dlatent_avg = Gs.get_var('dlatent_avg') # [component]\n",
        "  for i in range(truncation_cutoff):\n",
        "    dlatent[0][i] = (dlatent[0][i]-dlatent_avg)*truncation_psi + dlatent_avg\n",
        "    \n",
        "  return dlatent\n",
        "\n",
        "def generate_ws_from_zs(zs, truncation_psi=0.7, truncation_cutoff=9):\n",
        "  ws = []\n",
        "  for z in zs:\n",
        "    ws.append( convertZtoW(z, truncation_psi, truncation_cutoff) )\n",
        "  return ws\n",
        "\n",
        "def interpolate(zs, steps):\n",
        "   out = []\n",
        "   for i in range(len(zs)-1):\n",
        "    for index in range(steps):\n",
        "     fraction = index/float(steps) \n",
        "     out.append(zs[i+1]*fraction + zs[i]*(1-fraction))\n",
        "   return out\n",
        "\n",
        "# Taken from https://github.com/alexanderkuk/log-progress\n",
        "def log_progress(sequence, every=1, size=None, name='Items'):\n",
        "    from ipywidgets import IntProgress, HTML, VBox\n",
        "    from IPython.display import display\n",
        "\n",
        "    is_iterator = False\n",
        "    if size is None:\n",
        "        try:\n",
        "            size = len(sequence)\n",
        "        except TypeError:\n",
        "            is_iterator = True\n",
        "    if size is not None:\n",
        "        if every is None:\n",
        "            if size <= 200:\n",
        "                every = 1\n",
        "            else:\n",
        "                every = int(size / 200)     # every 0.5%\n",
        "    else:\n",
        "        assert every is not None, 'sequence is iterator, set every'\n",
        "\n",
        "    if is_iterator:\n",
        "        progress = IntProgress(min=0, max=1, value=1)\n",
        "        progress.bar_style = 'info'\n",
        "    else:\n",
        "        progress = IntProgress(min=0, max=size, value=0)\n",
        "    label = HTML()\n",
        "    box = VBox(children=[label, progress])\n",
        "    display(box)\n",
        "\n",
        "    index = 0\n",
        "    try:\n",
        "        for index, record in enumerate(sequence, 1):\n",
        "            if index == 1 or index % every == 0:\n",
        "                if is_iterator:\n",
        "                    label.value = '{name}: {index} / ?'.format(\n",
        "                        name=name,\n",
        "                        index=index\n",
        "                    )\n",
        "                else:\n",
        "                    progress.value = index\n",
        "                    label.value = u'{name}: {index} / {size}'.format(\n",
        "                        name=name,\n",
        "                        index=index,\n",
        "                        size=size\n",
        "                    )\n",
        "            yield record\n",
        "    except:\n",
        "        progress.bar_style = 'danger'\n",
        "        raise\n",
        "    else:\n",
        "        progress.bar_style = 'success'\n",
        "        progress.value = index\n",
        "        label.value = \"{name}: {index}\".format(\n",
        "            name=name,\n",
        "            index=str(index or '?')\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup 4: Define some more python functions\n",
        "\n",
        "These are functions I wrote, to make it more straightforward to do the kinds of things I'm wanting students to do.\n",
        "\n",
        "Take a minute to look at the step-by-step working of `show_seeds()`"
      ],
      "metadata": {
        "id": "JCJRXgONL9gV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_seeds(seeds):\n",
        "  # Use each seed to initialize the rng and sample latent vectors:\n",
        "  # 512 random variables from a Standard Normal distribution\n",
        "  zs = generate_zs_from_seeds(seeds)\n",
        "\n",
        "  # Map each latent vector into W-space that the generator accepts as input\n",
        "  ws = generate_ws_from_zs(zs)\n",
        "\n",
        "  # Push each W through the generator to yield an image\n",
        "  images = generate_images_in_w_space(ws)\n",
        "\n",
        "  # concatenate the images into a bigger gridded image\n",
        "  grid = createImageGrid(images, scale=0.7, rows=3)\n",
        "\n",
        "  # show them all!\n",
        "  imshow(grid)\n",
        "\n",
        "def seeds2imgs(seeds):\n",
        "  return generate_images_in_w_space( generate_ws_from_zs( generate_zs_from_seeds( seeds ) ) )\n",
        "\n",
        "def seed2img(seed):\n",
        "  imgs = seeds2imgs([seed])\n",
        "  return imgs[0]\n",
        "\n",
        "def show_seed_img(seed):\n",
        "  imshow(seed2img(seed))\n",
        "\n",
        "def z2img(latent):\n",
        "  imgs = generate_images_in_w_space( generate_ws_from_zs( [latent] ) )\n",
        "  return imgs[0]\n",
        "\n",
        "def show_z_img(latent):\n",
        "  imshow(z2img(latent))\n",
        "\n",
        "# For instance if you want to save a latent vector, dump(m_fave_z, 'myZ.pkl')\n",
        "# Then DOWNLOAD a copy of it, or it will be recycled along with the \n",
        "# ephemeral cloud computer\n",
        "def dump(thing, path):\n",
        "  with open(path, 'wb') as file:\n",
        "    pickle.dump(thing, file)\n",
        "\n",
        "# In a later Colab session, upload your file and you can load it back in\n",
        "# my_fave_z = load('myZ.pkl')\n",
        "def load(path):\n",
        "  with open(path, 'rb') as file:\n",
        "    thing = pickle.load(file)\n",
        "  return thing\n"
      ],
      "metadata": {
        "id": "Dpqf_-GvqrS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate lots of random faces\n",
        "\n",
        "This cell uses chooses 9 random numbers and holds them in a list called `seeds`\n",
        "\n",
        "Those `seeds` are then passed into `generate_images_from_seeds`, which uses each seed to initialize a standard normal (`np.random.randn`) rng, and then use that to generate a 'latent' vector z of 512 N(0,1) samples. z is mapped into w-space, and w is input to the Generator network, yielding an output image.\n",
        "\n",
        "By this process, any integral seed can repeatably control the rng to generate the same latent vector z, which maps to the same w-vector, which causes the neural network to generate the same image.\n",
        "\n",
        "Take note of the extremely wide variety of faces generated! Gender, race, age, hairstyles/hats, glasses/earrings/etc. Once in a while, though, it goofs and there are noticeable defects, which often appear as water droplets, or in extreme cases, globs of electric blue goo."
      ],
      "metadata": {
        "id": "Rx9rpAluMUeo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate some random seeds\n",
        "seeds = np.random.randint(10000000, size=9)\n",
        "print(seeds)\n",
        "\n",
        "show_seeds(seeds)"
      ],
      "metadata": {
        "id": "UtLfilNLqGxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Z Interpolation\n",
        "\n",
        "As seen above, an image can be generated starting from just a single number used to seed the rng, but what counts more is the latent vector z. \n",
        "\n",
        "It turns out, if you have any two latent vectors $z1$ and $z1$ (which generate two images img1 and img2), halfway between those vectors is an image which is 'halfway' between the images! Or $$z=\\frac{1}{10}z1 + \\frac{9}{10}z2$$ generates an image which is 10% of the way from the first image to the second, etc. \n",
        "\n",
        "A smooth path between the latent vectors, generates a smooth transition of generated images, which this next cell demonstrates."
      ],
      "metadata": {
        "id": "eIOEmPTQMOwu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_aZvophLZQOw"
      },
      "outputs": [],
      "source": [
        "# Simple (Z) interpolation\n",
        "\n",
        "# these are the two seeds Christensen left in the notebook\n",
        "seeds = [5015289 , 9148088] # that hat tho!\n",
        "\n",
        "# You can try out any other two numbers you want, or let this choose randomly:\n",
        "seeds = np.random.randint(10000000, size=2)\n",
        "\n",
        "# print the seeds in case you see an interesting face; just jot down the \n",
        "# seed and you'll be able to regenerate it later!\n",
        "print(seeds)\n",
        "\n",
        "# This does the same thing above to rng two latent vectors 'z' using the seeds\n",
        "z1, z2 = generate_zs_from_seeds(seeds)\n",
        "\n",
        "# The interpolate() function scales linearly from z1 to z2, and this renders \n",
        "# the resulting sequence of images\n",
        "number_of_steps = 9\n",
        "number_of_rows  = 3\n",
        "imgs = generate_images(interpolate([z1,z2], number_of_steps), 1.0)\n",
        "imshow(createImageGrid(imgs, 0.4 , number_of_rows))\n",
        "# If you want, you could change that from 9 steps, shown in rows of 3, to \n",
        "# 16 steps, shown in rows of 4, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYdsgv4i6YPl"
      },
      "source": [
        "# What Z generates YOUR face?\n",
        "\n",
        "StyleGAN2 comes with a projector that finds the closest generatable image based on any input image. This allows you to get a feeling for the diversity of the portrait manifold.\n",
        "\n",
        "^That sentence is courtesy Mikael Christensen. What it means is, give it a picture of YOUR face (or maybe a celebrity you like), it will search to find a latent vector Z that generates an image as close as possible to that target image."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Find Your Face Step 1: Upload target image\n",
        "\n",
        "Look over to the left, in the Files sidebar. Expand directories **stylegan2/projection/target/.** Drag the image onto the target directory. \n",
        "\n",
        "\n",
        "Only **ONE** image may be in the directory.\n",
        "\n",
        "The image needs to be a color .png, with a size of exactly 1024x1024 pixels. The\n",
        "image needs to have the eyes/mouth fairly-well aligned with the starting image \n",
        "for the search. A rule of thumb for that is to have about a finger's \n",
        "width of space below the chin, and a little bit of hair cut off the top. "
      ],
      "metadata": {
        "id": "Mn6__cz5Ujz6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Find Your Face Step 2: Find Your Face\n",
        "\n",
        "Run the cell below, which uses to the projector to execute the search.\n",
        "\n",
        "After a few seconds, you will see `0 / 1000`, and from there it takes about a second per step, so It will take maybe 10-15 minutes to complete.\n",
        "\n",
        "Unfortunately, you can't just walk away, because if the Colab Notebook finishes and then times out, it will be recycled, and your output along with it.\n",
        "\n",
        "While it's crunching, if you're curious, open up stylegan2/projection/out, and double-click on any image-stepNNNN.png to take a peek at its progress!"
      ],
      "metadata": {
        "id": "rG0zC0dbUlQk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDLJBbpz6n4k"
      },
      "outputs": [],
      "source": [
        "# Convert uploaded images to TFRecords\n",
        "import dataset_tool\n",
        "dataset_tool.create_from_images(\"./projection/records/\", \"./projection/target/\", True)\n",
        "\n",
        "# Run the projector\n",
        "import run_projector\n",
        "import projector\n",
        "import training.dataset\n",
        "import training.misc\n",
        "import os \n",
        "\n",
        "print('Loading images from \"%s\"...' % 'records')\n",
        "dataset_obj = training.dataset.load_dataset(data_dir='projection', \n",
        "                                            tfrecord_dir='records', \n",
        "                                            max_label_size=0, \n",
        "                                            verbose=True, \n",
        "                                            repeat=False, \n",
        "                                            shuffle_mb=0)\n",
        "assert dataset_obj.shape == Gs.output_shape[1:]\n",
        "\n",
        "\n",
        "# Here we set up and use the projector\n",
        "proj = projector.Projector()\n",
        "proj.set_network(Gs)\n",
        "\n",
        "# for a full run this should be 1000, but it can be set smaller for testing\n",
        "proj.num_steps = 1000\n",
        "\n",
        "images, _labels = dataset_obj.get_minibatch_np(1)\n",
        "images = training.misc.adjust_dynamic_range(images, [0, 255], [-1, 1])\n",
        "run_projector.project_image(proj, \n",
        "                            targets=images, \n",
        "                            png_prefix='projection/out/image-', \n",
        "                            num_snapshots=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Find Your Face: Step 3: Make a video\n",
        "\n",
        "Once the finding is done, the directory stylegan2/projection/out/ is full of images named `image-step0NNNN.png`. Because of the  \n",
        "`num_snapshots=100`, it grabbed every tenth generate image on the journey to \n",
        "finding your face.\n",
        "\n",
        "This cell reads crunches all those together and saves out as a mp4 video file,\n",
        "each frame is the concatenation of the fixed target image on the left, and the\n",
        "current search step on the right.\n",
        "\n",
        "If you want the saved filename to be something different than `movie.mp4`, you can edit the `movieName` variable."
      ],
      "metadata": {
        "id": "wH26aJUSWD_D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OmjPpjFU6yq3"
      },
      "outputs": [],
      "source": [
        "# Create video \n",
        "\n",
        "import glob\n",
        "\n",
        "imgs = sorted(glob.glob(\"projection/out/*step*.png\"))\n",
        "\n",
        "target_imgs = sorted(glob.glob(\"projection/out/*target*.png\"))\n",
        "assert len(target_imgs) == 1, \"More than one target found?\"\n",
        "target_img = imageio.imread(target_imgs[0])\n",
        "\n",
        "movieName = \"projection/movie.mp4\"\n",
        "\n",
        "with imageio.get_writer(movieName, mode='I') as writer:\n",
        "    for filename in log_progress(imgs, name = \"Creating animation\"):\n",
        "        image = imageio.imread(filename)\n",
        "\n",
        "        # Concatenate images with original target image\n",
        "        w,h = image.shape[0:2]\n",
        "        canvas = PIL.Image.new('RGBA', (w*2,h), 'white')\n",
        "        canvas.paste(Image.fromarray(target_img), (0, 0))\n",
        "        canvas.paste(Image.fromarray(image), (w, 0))\n",
        "\n",
        "        writer.append_data(np.array(canvas))  "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Find Your Face: Step 4: Save your video\n",
        "\n",
        "Go find your video file on the File sidebar on the left, click '...', and Download. If you didn't change the filename away from movie.mp4, you have\n",
        "another opportunity to change the filename when you save the download."
      ],
      "metadata": {
        "id": "bSpp4Hc2Xfac"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Find Your Face: Step 5a: Save your W-space vector\n",
        "\n",
        "The next cell will save the W-space vector that the search finally settled on.\n",
        "Later you can reload it and display it again. The StyleGAN2 code typically calls\n",
        "this `dlatent` vs just `latent` for z\n",
        "\n",
        "***IMPORTANT*** Download a copy of this file! If you don't download it and the Colab session times out, it will be gone!"
      ],
      "metadata": {
        "id": "dBYjJY4J2kVQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uk6aBG7H5I3G"
      },
      "outputs": [],
      "source": [
        "myW = proj.get_dlatents()\n",
        "\n",
        "# You can change this filename if you want\n",
        "filename = 'myWvector.pkl'\n",
        "\n",
        "dump(myW, filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Find Your Face: Step 5b: Reload your W-space vector\n",
        "\n",
        "If you are returning to this notebook with a saved W-space vector, you can load it back in, instead of the long process to search for a near-match.\n",
        "\n",
        "First upload your saved file to the colab session."
      ],
      "metadata": {
        "id": "3QhuYvXU31SD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Change this filename to match what you uploaded\n",
        "filename = 'myWvector.pkl'\n",
        "\n",
        "myW = load(filename)"
      ],
      "metadata": {
        "id": "WYEH2nrZ4U4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Find Your Face: Step 6: Save your image\n",
        "\n",
        "In addition to the side-by-side video from above, you can also save just an image."
      ],
      "metadata": {
        "id": "cP6_-A3J5IQW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvLEQCI96Med"
      },
      "outputs": [],
      "source": [
        "# This actually creates a list of images\n",
        "imgs = generate_images_in_w_space([myW])\n",
        "me = imgs[0]\n",
        "imshow(me)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# change this if you want\n",
        "imgFilename = 'me.png'\n",
        "\n",
        "me.save(imgFilename)"
      ],
      "metadata": {
        "id": "K_EXh65z6Z2K"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "StyleGAN2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}