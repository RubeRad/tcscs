{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/RubeRad/tcscs/blob/master/CovidPlots.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77-aZCKD1c4Z"
   },
   "source": [
    "# Introduction to Python, Jupyter, Pandas, and Matplotlib\n",
    "This is a Jupyter Python notebook, which is a collection of cells. Each cell is either of type 'markdown' (formatted text, like this cell) or code (python, grey background). The two most important rules of Jupyter Notebooks are:\n",
    "1. ***SHIFT-ENTER*** will cause the current cell to execute. \n",
    "  - For Markdown cells, 'execute' means render the formatting. ([Here's a markdown cheatsheet](https://sqlbak.com/blog/wp-content/uploads/2020/04/Jupyter-Notebook-Markdown-Cheatsheet.pdf))\n",
    "  - For Code cells, 'execute' means run the python.\n",
    "  - Some Code cells take a while to execute, watch for the * to change to a number\n",
    "1. Any cell can be edited (double-click into it) and re-executed (SHIFT-ENTER again).\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xZid-1MF1c4a"
   },
   "source": [
    "The first code in any Python script/Jupyter notebook, needs to import any libraries that will be used. The `as` directives allow specification of nicknames that are more convenient to type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6YLF-sx31c4b"
   },
   "outputs": [],
   "source": [
    "from datetime import date        # Because we're going to be plotting time series\n",
    "import matplotlib.pyplot as plt  # This is for creating graphs\n",
    "import pandas            as pd   # This greatly simplifies handling of tabular (csv) data\n",
    "\n",
    "# this is one special little function to support graphs with separate left/right scales\n",
    "from mpl_toolkits.axes_grid1 import host_subplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Date handling in python\n",
    "There's a lot more we could go into regarding time zones, subsecond precision, etc, but for this dataset we only need to deal with calendar dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What answer do you expect for this?\n",
    "date1 = '2023-01-30'\n",
    "date2 = '2023-02-06'\n",
    "date2 - date1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The datetime module lets us do this and get the right answer\n",
    "date1 = date(2023,2,25)  # what if it's 2020?\n",
    "date2 = date(2023,3,5)\n",
    "date2 - date1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_En7DUu1c4c"
   },
   "source": [
    "# Slurp the data into a pandas DataFrame\n",
    "\n",
    "The `pandas` function `read_csv()` can read .csv files on your computer, but it's so smart it can even slurp in a .csv directly from online. This reads in the csv data linked to on this page: https://ourworldindata.org/coronavirus-source-data). \n",
    "\n",
    "It takes a few seconds to download, watch for the `*` to change to a number. \n",
    "\n",
    "**The two most important terms in the grammar of pandas are `DataFrame` and `Series`/column**. A `DataFrame` is equivalent to  one tab of a spreadsheet (or one worksheet of a workbook). Each column of the `DataFrame` is one `Series`. The data object returned by `read_csv()` is held in a variable named `df` which stands for `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W-3fNc6a1c4c"
   },
   "outputs": [],
   "source": [
    "url = 'https://covid.ourworldindata.org/data/owid-covid-data.csv'\n",
    "dfall = pd.read_csv(url)  # this takes a few seconds because it has to download the file\n",
    "#dfall = pd.read_csv(url, parse_dates=['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BFZxJDm81c4c"
   },
   "source": [
    "As usual, if you mention any python variable at the end of a code cell, the notebook will try to print or summarize it. The `NaN` ('not a number') are empty cells in the csv (missing data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jKb_jonk1c4c"
   },
   "outputs": [],
   "source": [
    "dfall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zGYGj0wF1c4d"
   },
   "source": [
    "That doesn't even have enough room to show all 67 columns in the data. What are all those columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RZoSDrt91c4d"
   },
   "outputs": [],
   "source": [
    "dfall.columns   # pandas assumes the first row of the csv are column headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bt0bc2ky1c4d"
   },
   "outputs": [],
   "source": [
    "# info is a 'class function', so it needs (); sometimes input parameters go in there.\n",
    "dfall.info() # Data size, column headers, counts, and types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q_uL2Zmt1c4e"
   },
   "source": [
    "## Exercise\n",
    "\n",
    "Note that the type listed for column `3 date` up there, is just `object` (i.e. text string). We want pandas to see that as type `datetime`. Go back up to the `read_csv(url)` line and uncomment the 2nd option to tell pandas to re-read the data that way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the data\n",
    "In Data Science, the majority of the work is obtaining, aggregating, and cleaning the data. This is a great dataset that doesn't need much work, but we should fill in some of the holes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# isnull() by itself prints too much stuff, sum() helps summarize\n",
    "dfall.isnull() #.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filling in missing values in a specific column\n",
    "`iso_code` and `date` have 0 missing entries, that's good. But we're going to be dividing by `population` so missing data is not good there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This gives us the list of True/False for where all the missing values are in the population column\n",
    "# The list is too long to see where those 1113 Trues are hiding\n",
    "dfall.population.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a slice that captures all the rows where population is missing\n",
    "df_no_pop = #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The fillna() command allows you to say what you want for the missing values\n",
    "dfall.population.fillna(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go back and check if that worked. \n",
    "\n",
    "* Go back up and re-execute the `dfall.isnull().sum()`; how many missing entries does `population` now have?\n",
    "* Re-execute the `df_no_pop` slice, how many rows are in it now?\n",
    "\n",
    "What happened is that the `fillna(1)` returned a version of the column that was filled in (which could have been caught in a separate variable), but didn't change the column. To actually change, add `, inplace=True` to the `fillna` arguments.\n",
    "\n",
    "## Filling in missing values in an entire DataFrame\n",
    "We had to be especially careful with the population column, because we're going to need to divide by it. But for the rest, it's ok to fill in missing values with 0. This is actually quite easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfall.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gtMQMz311c4g"
   },
   "source": [
    "# Slicing a DataFrame down to a subset of columns\n",
    "67 is way more columns than we need for this exercise! Just like in the last notebook we learned about *slicing* a `DataFrame` to get a `DataFrame` for a subset of the rows, we can also slice a smaller `DataFrame` of desired `Series`/columns. As always in Python, take careful note of the syntax, with `[]` inside `[]`, and quotes and commas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zNAYQw-t1c4g",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dfall is all the data, let's call this smaller DataFrame df\n",
    "df = dfall[  ['iso_code', 'location', 'date', 'new_cases', 'new_deaths', 'total_cases', 'population']  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "In the following cell, try all these ways of inspecting the smaller `DataFrame`:\n",
    "* `df`\n",
    "* `df.head()`\n",
    "* `df.tail()`\n",
    "* `df.describe()`\n",
    "* `df.info()`\n",
    "* `df.isnull().sum()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OAz5td2K1c4e"
   },
   "source": [
    "# Choosing individual columns by name\n",
    "\n",
    "Any individual `Series` (column) can be fetched out of the `DataFrame` one of two ways:\n",
    "* `df['iso_code']` always works\n",
    "* `df.iso_code` works if the column name has no spaces or weird characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Z7NDlB71c4f"
   },
   "outputs": [],
   "source": [
    "df.iso_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fCHIP5431c4f"
   },
   "outputs": [],
   "source": [
    "df.iso_code.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F5-dK1J71c4f"
   },
   "outputs": [],
   "source": [
    "df.iso_code.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lYsFeCeu1c4f"
   },
   "outputs": [],
   "source": [
    "df.iso_code.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TcV1VECz1c4f"
   },
   "outputs": [],
   "source": [
    "df.iso_code.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F9pzVd4X1c4f"
   },
   "source": [
    "## Exercise\n",
    "* Go back into those cells above, and edit to investigate some other Series, like 'location', 'date', 'new_cases', 'population'\n",
    "* 'value_counts()' has different top numbers for `iso_code` vs `location` vs `date`. What does that mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fsQuJMq91c4g"
   },
   "source": [
    "# Slicing DataFrame rows with a condition\n",
    "A `DataFrame` can be 'sliced' (filtered) based on conditions applied to the data. These actions can be read something like \"The new variable USA is a `DataFrame` made from df by selecting all the rows for which the 'iso_code' is equal to the text constant 'USA'\"\n",
    "\n",
    "***Critically important Python NOTE:*** One = means the action **assignment**, whatever is on the right, put it into the left. It is the same as Snap's `Set <variable> to <value>` from the  yellow Variables tab.  Two == means the **question** *are these two things the same* (or in this context, *where* are these two the same?), and is equivalent to the Predicate = in Snap!, from the green Operators tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r03FO_zJ1c4g"
   },
   "outputs": [],
   "source": [
    "# this is a Series of True/False; it's True in all the rows with iso_code=='USA'\n",
    "df.iso_code == 'AFG' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# catch that list of True/False in a variable\n",
    "AFGrows = (df.iso_code == 'AFG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zLGIZEpQ1c4h"
   },
   "source": [
    "Once we have a list of True/False flags for all the rows, we can use that to filter the DataFrame to a smaller DataFrame with just the rows flagged True. Note that the resulting DataFrame is all `iso_code==AFG`, and fewer rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OdOqswx41c4h"
   },
   "outputs": [],
   "source": [
    "AFG = df[ AFGrows ]\n",
    "AFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This can all be done in one line,\n",
    "# without the intermediate variable AFGrows\n",
    "AFG = df[ df.iso_code == 'AFG' ]\n",
    "AFG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TdUV-Cpj1c4h"
   },
   "source": [
    "Note we can do the above steps in one line per country, like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x1ZNRSgj1c4h"
   },
   "source": [
    "## Exercise\n",
    "Use the following cells to make a USA sliced DataFrame, and inspect it like USA.info(), USA.describe(), etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "18kP08e_1c4h"
   },
   "outputs": [],
   "source": [
    "# Make a slice of all the rows for the USA\n",
    "USA = #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O3R2nF1x1c4h"
   },
   "outputs": [],
   "source": [
    "# Use various methods to inspect the USA DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oI1tCB6F1c4h"
   },
   "source": [
    "# Plotting Pandas Series' with MATPLOTLIB\n",
    "Now that we have a handle on manipulating csv data with pandas, we turn to the main point, which is to be able to visualize the data graphically. Here's a naive plot to start with. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vf1gDHQx1c4i"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,8)) # 16x8 is twice as wide as tall\n",
    "ax = plt.gca()\n",
    "\n",
    "ax.plot(USA.date, USA.new_cases)\n",
    "\n",
    "#ax.set_xlim( date(2022,1,1), date(2022,3,1) )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Why does it look so jaggedy like that? use the `ax.set_xlim()` function (as in the MatplotlibIntro notebook) and the `date` objects (as in the top of this notebook) to narrow down the graph above to a couple-month range and get a closer look."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data smoothing\n",
    "The USA curve (and all the curves) are jaggedy because bureaucrats don't file paperwork on the weekends. Pandas lets us create a smoother dataset with a rolling window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable name \"USAcs\" is \"USA new Cases (Smoothed)\"\n",
    "# Try this first as-is, and then with the .mean() \n",
    "USAcs = USA.new_cases.rolling(window=7) # .mean()\n",
    "USAcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now go copy that matplotlib code from above into the cell below, and add a plot for this new smoothed dataset `USAcs`. Then you can remove the `set_xlim` and see the big picture. (And then you can remove the plot of the jaggedy `USA.new_cases` series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decorating with annotations\n",
    "There are a lot of ways to annotate a matplotlib graph, some of which we've seen before:\n",
    "* `ax.text(x,y,s)`  at position x,y, write text string s\n",
    "* `ax.vlines(x, ymin, ymax)` at x (or list of xs), draw vertical line from ymin to ymax\n",
    "* `ax.hlines(y, xmin, xmax)` at y (or list of ys), draw horizontal line from xmin to xmax\n",
    "* `plt.axvline(x)` draw full vertical line at x  (or fractional with `ymin` and `ymax` between 0..1)\n",
    "* `plt.axhline(y)` draw full horizontal line at y (or fractional with `xmin` and `xmax` between 0..1)\n",
    "* `ax.annotate(s, (x,y) )` basically the same as `ax.text(x,y,s)`\n",
    "  * But you can add optional arguments `xytext=(x,y), arrowprops=dict(arrowstyle='->',color='r'))` to displace the text label to `xytext` and point the annotated `(x,y)` with an arrow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell and see what happens:\n",
    "ax.text()         # NOTE: () with nothing in them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run this cell and see what happens:\n",
    "help(plt.vlines)  # NOTE: no ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mark up the following graph:\n",
    "* Use `plt.axvline()` to make red lines to mark the dates when the classes of 2021 and 2022 did this notebook, `date(2020,12,9)` and `date(2021,12,8)`\n",
    "* Use `ax.text()` to write \"Year 1\" and \"Year 2\" next to those lines.\n",
    "* Use `ax.annotate()` to write \"Omicron\" with an arrow pointing to the Christmas 2021 spike."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,8)) # 16x8 is twice as wide as tall\n",
    "ax = plt.gca()\n",
    "\n",
    "ax.plot(USA.date, USAcs)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dual/Twin axes\n",
    "Use the cells below to also plot `new_deaths` for the USA. The scale of the `new_deaths` numbers is so much smaller than `new_cases`, no detail is visible! It is necessary to scale the `new_deaths` with a separate Y-axis on the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this cell to construct a data Series for USA new Deaths (Smoothed)\"\n",
    "USAds = #..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once USAds is working above, uncomment the first plot()\n",
    "# to plot both series together\n",
    "\n",
    "fig = plt.figure(figsize=(16,8)) # 16x8 is twice as wide as tall\n",
    "ax = plt.gca()\n",
    "\n",
    "#ax.plot(USA.date, USAcs)\n",
    "ax.plot(USA.date, USAds)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step one: twinx()\n",
    "Instead of plotting both curves on `ax = plt.gca()`, make a separate `ax2 = ax.twinx()` and use that to plot `death_cases`.\n",
    "\n",
    "How/why does that work? `ax2` has the same (twinned) X-axis as `ax`, but it makes a separate Y-axis on the right\n",
    "\n",
    "## Step two: differentiate with colors\n",
    "Which blue curve is which? Why are they both blue? (Because `ax2` starts over with default coloring)\n",
    "\n",
    "* Specify the colors in the `plot` commands\n",
    "* Use `set_ylabel()` for both `ax` and `ax2`, and also specify `color=` to match the plot lines\n",
    "\n",
    "## Step three: add a Legend\n",
    "\n",
    "* Easier way: `fig.legend(['one','two'])` -- but because this is a *figure* legend, it is complicated to force it inside the Axes\n",
    "* Harder way: Replace `ax = plt.gca()` with `ax = host_subplot(111)`, and then instead of `fig.legend` do `ax.legend`. This method requires the special import statement up top, to bring in `host_subplot`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing countries per-capita\n",
    "Prepare a sliced, smoothed dataframes like above, for ITA (Italy), or some other country of your choosing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITA = # slice data frame for rows where iso_code == 'ITA'\n",
    "ITAcs = # ITAly new Cases (Smoothed)\n",
    "ITAds = # ITAly new Deaths (Smoothed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,8)) # 16x8 is twice as wide as tall\n",
    "ax = plt.gca()\n",
    "\n",
    "ax.plot(USA.date, USAcs)\n",
    "ax.plot(ITA.date, ITAcs)\n",
    "\n",
    "ax.legend(['USA','Italy'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F8NffThP1c4i"
   },
   "source": [
    "Those curves are obviously incomparable because of quite different country populations. We can convert to per-million by applying arithmetic to whole series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sr9JgmwR1c4i"
   },
   "outputs": [],
   "source": [
    "# Try each of these, one at a time\n",
    "USAcs                               # the whole number of USA cases per day (smoothed)\n",
    "#USAcs / USA.population             # row-by-row divide to get per capita\n",
    "#USAcs / USA.population * 1000000   # per capita is too small, so scale up to per million"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two data series here which are USA/ITAly smoothed cases, per Million\n",
    "USAcsm = # ...\n",
    "ITAcsm = # ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now copy the plotting code from above and plot the USAcsm and ITAcsm curves together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework\n",
    "\n",
    "Choose four countries (two of them can be USA and ITAly), and make a figure with two subplots above and below (refer to Anscombe's Quartet for subplots: above and below are 2x1, so the `fig.add_subplot()` codes would be `211` and `212`)\n",
    "\n",
    "Here is a table of `iso_code` abbreviations that you might find helpful (but you don't have to pick from these)\n",
    "\n",
    "|iso_code  | location |iso_code  | location |\n",
    "|:---------|:---------|:---------|:---------|\n",
    "|AUS       | Australia|IRN       | Iran     |\n",
    "|BRA       | Brazil   |ISR       | Israel   |\n",
    "|CAN       | Canada   |MEX       | Mexico   |\n",
    "|ESP       | Spain    |NZL       | New Zealand |\n",
    "|FRA       | France   |RUS       | Russia   |\n",
    "|GBR       | United Kingdom |ZAF | South Africa |\n",
    "\n",
    "The subplot above should be `new_cases` for the four countries (smoothed, per million), and the subplot below should be `new_deaths` for the four countries (smoothed, per million).\n",
    "\n",
    "The whole figure should be well-designed, styled, colored, labeled, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this cell to prepare the new_cases/deaths smoothed, per-million Series for each country\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u0i2MvMk1c4i"
   },
   "outputs": [],
   "source": [
    "# Use this cell to make your figure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QtwgLb_g1c4j"
   },
   "source": [
    "# World Aggregation\n",
    "We can add all the countries together. `groupby(['date']).sum()` says we want a new `DataFrame`, with a row for every unique date, and all the other columns are added up per date.\n",
    "\n",
    "(For some situations, it might make more sense to `groupby(['date']).mean()` or `min()` or `max()` or `median()`, and there are many other grouping options!)\n",
    "\n",
    "Note how `world.head()` or `tail()` prints `date` differently than the column headers, and `date` doesn't even show up in the columns; that's because it is now the 'index', not a regular column. \n",
    "\n",
    "Repeat the cell with `.reset_index()` active, and you will see that date is set to a regular column again (and the index is just a running counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q3bRjLjB1c4j",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "world = df.groupby(['date']).sum()  #.reset_index()\n",
    "# ignore the warnings\n",
    "# or as the warning suggests, put numeric_only=True inside the sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world.tail()  # is 'date' formatted differently?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world.columns # does it include 'date'?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ha_u-hMw1c4k"
   },
   "source": [
    "After the `.reset_index()` above is applied, the following code should make a barebones graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bijgaAbD1c4k"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.gca()\n",
    "\n",
    "ax.plot(world.date, world.new_cases)\n",
    "ax.plot(world.date, world.new_deaths)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qI7Ao7kL1c4k"
   },
   "source": [
    "# Homework\n",
    "Improve the graph above in the following ways:\n",
    "1. Initialize the figure to have a larger figsize with an attractive aspect ratio.\n",
    "1. Use a 7-day rolling average instead of the raw numbers\n",
    "1. Use set_ylabel() to describe the left and right axes, and use set_title() to title the whole chart\n",
    "1. Use different colors for the cases/deaths graphs\n",
    "1. Use plt.legend()\n",
    "1. Use the plt.axvline() example above to annotate a few significant dates, such as the start of vaccination, the discovery of the Delta/Omicron variants, etc.\n",
    "1. Decorate the graph with text, annotations, etc, highlighting significant events during the progress of the pandemic. Some ideas: release of the vaccine, waves like Delta/Omicron, date of 1M US deaths, dates public figures got infected or died, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MnKaRSCK1c4l"
   },
   "source": [
    "---\n",
    "# Optional extra: Shifted Dates\n",
    "\n",
    "Most complicated, we can see that these curves would be more comparable if they were date-shifted, to reflect the different times when the pandemic hit different countries. A common technique is to line them all up based on when they had a certain common minimum number of cases, say 10. We will filter on a condition again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ImVxqJLQ1c4l"
   },
   "outputs": [],
   "source": [
    "# I could just type 100 in every line below, but this way if I want to experiment with a different value\n",
    "# I can edit just 1 line, instead of having to edit a line for every country (especially as countries are added)\n",
    "min_cases = 100\n",
    "USAsh = USA[ USA['total_cases'] >= min_cases ]  # 'sh' for shift\n",
    "ITAsh = ITA[ ITA['total_cases'] >= min_cases ]\n",
    "SWEsh = SWE[ SWE['total_cases'] >= min_cases ]\n",
    "KORsh = KOR[ KOR['total_cases'] >= min_cases ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BaAqR3gW1c4m"
   },
   "outputs": [],
   "source": [
    "USAsh['date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I0L6v3Ey1c4m"
   },
   "outputs": [],
   "source": [
    "USAsh['date'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UO2T4RSI1c4m"
   },
   "outputs": [],
   "source": [
    "ITAsh['date'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xKwtkZnL1c4m"
   },
   "outputs": [],
   "source": [
    "USAsh['date'].min() - ITAsh['date'].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LrHEBICE1c4m"
   },
   "source": [
    "Now we can see above that Italy reached 100 cases on Feb 23, 10 days before the US on Mar 4. (And that date objects can be subtracted!)\n",
    "\n",
    "Here are all the dates where these countries reached 100 cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eXLU8POh1c4m"
   },
   "outputs": [],
   "source": [
    "USAt0 = USAsh['date'].min()\n",
    "ITAt0 = ITAsh['date'].min()\n",
    "SWEt0 = SWEsh['date'].min()\n",
    "KORt0 = KORsh['date'].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0XuEqPz_1c4n"
   },
   "source": [
    "Just like we were able to simply multiply and divide the entire 'new_cases' Series by constant numbers, we can subtract the start date from the date Series, yielding number of days since 100 cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5djyfH3a1c4n"
   },
   "outputs": [],
   "source": [
    "USAsh['date'] - USAt0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4kKf3V4y1c4n",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "USAshX = USAsh['date'] - USAt0\n",
    "USAshY = USAsh['new_cases'].rolling(window=7).mean()/USAsh.population*1000000\n",
    "plt.figure(figsize=(16,8))\n",
    "ax = plt.gca()\n",
    "ax.plot(USAshX, USAshY)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Kh-q5kl1c4n"
   },
   "source": [
    "Note that plot goes from 0 to 2.5e16. Even though the description of `USAsh['date'] - USAt0` above says 'days', matplotlib is interpreting it as milliseconds. We can fix this by forcing conversion to days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e91n-dq91c4n"
   },
   "outputs": [],
   "source": [
    "USAshX = (USAsh['date'] - USAt0).astype('timedelta64[D]')   # 'D' is for Days\n",
    "USAshX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0LE82VBv1c4o",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "ax = plt.gca()\n",
    "ax.plot(USAshX, USAshY)\n",
    "ax.plot((ITAsh['date']-ITAt0).astype('timedelta64[D]'), ITAsh['new_cases'].rolling(window=7).mean()/ITAsh.population*1000000)  \n",
    "ax.plot((SWEsh['date']-SWEt0).astype('timedelta64[D]'), SWEsh['new_cases'].rolling(window=7).mean()/SWEsh.population*1000000) \n",
    "ax.plot((KORsh['date']-KORt0).astype('timedelta64[D]'), KORsh['new_cases'].rolling(window=7).mean()/KORsh.population*1000000)\n",
    "ax.legend(['USA', 'Italy', 'Sweden', 'South Korea'])\n",
    "ax.set_xlim(0, 300)\n",
    "ax.set_ylim(0, 1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i12dvJ8Z1c4o"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
