{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Python, Jupyter, Pandas, and Matplotlib\n",
    "This is a Jupyter Python notebook, which is a collection of cells. Each cell is either of type 'markdown' (formatted text, like this cell) or code (python, grey background). The two most important rules of Jupyter Notebooks are:\n",
    "1. ***SHIFT-ENTER*** will cause the current cell to execute. \n",
    "  - For Markdown cells, 'execute' means render the formatting. ([Here's a markdown cheatsheet](https://sqlbak.com/blog/wp-content/uploads/2020/04/Jupyter-Notebook-Markdown-Cheatsheet.pdf))\n",
    "  - For Code cells, 'execute' means run the python.\n",
    "  - Some Code cells take a while to execute, watch for the * to change to a number\n",
    "1. Any cell can be edited (double-click into it) and re-executed (SHIFT-ENTER again).\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first code in any Python script/Jupyter notebook, needs to import any libraries that will be used. The `as` directives allow specification of nicknames that are more convenient to type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date        # Because we're going to be plotting time series\n",
    "import matplotlib.pyplot as plt  # This is for creating graphs\n",
    "import pandas            as pd   # This greatly simplifies handling of tabular (csv) data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pandas` function `read_csv()` can read .csv files on your computer, but it's so smart it can even slurp in a .csv directly from online. This reads in the csv data linked to on this page: https://ourworldindata.org/coronavirus-source-data). \n",
    "\n",
    "It takes a few seconds to download, watch for the `*` to change to a number. \n",
    "\n",
    "**The two most important terms in the grammar of pandas are `Series` and `DataFrame`**. A `DataFrame` is equivalent to  one tab of a spreadsheet (or one worksheet of a workbook). Each column of the `DataFrame` is one `Series`. The data object returned by `read_csv()` is held in a variable named `df` which stands for `DataFrame`.\n",
    "\n",
    "Note: Python is happy with specifying text constants (strings) with either single or double quotes, but single are preferred because they read cleaner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://covid.ourworldindata.org/data/owid-covid-data.csv'\n",
    "df  = pd.read_csv(url)  # this takes a few seconds because it has to download the file\n",
    "#df  = pd.read_csv(url, parse_dates=['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing you can do it Jupyter Notebooks that you can't do in regular Python scripts (programs), is to just mention something at the end of a code cell, which is a request for the notebook to display it. If it's too big to be printed completely, it will be automatically summarized. For starters, you can try to get a glimpse of the whole `DataFrame` object itself. The `NaN` ('not a number') are empty cells in the csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are a few more ways to describe/understand the data, execute each cell and take a minute to read and understand what information is provided:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape     # note shape is a 'data member', a variable that belongs to every data frame. (Rows, Columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns   # pandas assumes the first row of the csv are column headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# info is a 'class function', so it needs (); sometimes input parameters go in there.\n",
    "df.info() # Data size, column headers, counts, and types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Note that the type listed for column `3 date` up there, is just `object`. For proper handling, we want pandas to see that as type `datetime`. Go back up to the `read_csv(url)` line and uncomment the 2nd option to tell pandas to re-read the data that way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe() # some common statistics on each column, note you can scroll to the right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head() # first few rows of data (default 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(8) # last few rows of data (we specify 8 here, just to show how)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing individual columns by name\n",
    "\n",
    "Any individual `Series` can be fetched out of the data frame using square brackets, and some of the same functions apply, as well as a few others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['iso_code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['iso_code'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['iso_code'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['iso_code'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['iso_code'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Go back into those cells above, and edit to investigate some other Series, like 'location', 'date', 'new_cases'\n",
    "\n",
    "Change the first line to `series = df['iso_code']`, and for the rest of the lines use the variable `series` instead of repeating `df['iso_code']` all the time. \n",
    "\n",
    "Then edit `'iso_code'` in the first line to other column names, and rerun the following cells to see different results.\n",
    "\n",
    "'value_counts()' has different top numbers for `iso_code` vs `location` vs `date`. What does that mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slicing a DataFrame down to a subset of columns\n",
    "A smaller `DataFrame` can be created from any group of `Series` that you choose. As always in Python, take careful note of the syntax, with `[]` inside `[]`, and quotes and commas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's call this smaller DataFrame df6, because we're selecting 6 columns of interest\n",
    "df6 = df[  ['iso_code', 'location', 'date', 'new_cases', 'new_deaths', 'total_cases']  ]\n",
    "df6.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering DataFrame rows with a condition\n",
    "A `DataFrame` can be 'sliced' (filtered) based on conditions applied to the data. These actions can be read something like \"The new variable USA is a `DataFrame` made from df6 by selecting all the rows for which the 'iso_code' is equal to the text constant 'USA'\"\n",
    "\n",
    "***Critically important Python NOTE:*** One = means the action **assignment**, whatever is on the right, put it into the left. It is the same as Snap's `Set <variable> to <value>` from the  yellow Variables tab.  Two == means the **question** *are these two things the same* (or in this context, *where* are these two the same?), and is equivalent to the Predicate = in Snap!, from the green Operators tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AFGrows = df6['iso_code'] == 'AFG' # this is a Series of 61245 False/True; it's True in all the rows with iso_code=='USA'\n",
    "AFGrows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have a list of True/False flags for all the rows, we can use that to filter the DataFrame to a smaller DataFrame with just the rows flagged True. Note that the resulting DataFrame is all `iso_code==AFG`, and fewer rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AFG = df6[ AFGrows ]\n",
    "AFG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note we can do the above steps in one line per country, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USA = df6[ df6['iso_code'] == 'USA' ] # Set variable USA to the DataFrame of the rows of df6 with iso_code==USA\n",
    "ITA = df6[ df6['iso_code'] == 'ITA' ]\n",
    "SWE = df6[ df6['iso_code'] == 'SWE' ]\n",
    "KOR = df6[ df6['iso_code'] == 'KOR' ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Use the following cell to try various looks at these sliced DataFrames, like USA.info() or SWE.tail(), etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "USA.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Pandas Series' with MATPLOTLIB\n",
    "Now that we have a handle on manipulating csv data with pandas, we turn to the main point, which is to be able to visualize the data graphically. Here's a naive plot to start with. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USAx = USA['date']      # grab the Series 'date'      out of the USA DataFrame into a variable called USAx\n",
    "USAy = USA['new_cases'] # grab the Series 'new_cases' out of the USA DataFrame into a variable called USAy\n",
    "plt.figure(figsize=(16,8))\n",
    "ax = plt.gca()\n",
    "ax.plot(USAx, USAy)\n",
    "\n",
    "# Uncomment these lines, one at a time, to see what they do\n",
    "#ax.plot(ITA['date'], ITA['new_cases']) # The same can be done inside the plot command\n",
    "#ax.plot(SWE['date'], SWE['new_cases'])\n",
    "#ax.plot(KOR['date'], KOR['new_cases'])\n",
    "#ax.legend(['USA', 'Italy', 'Sweden', 'South Korea'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First off, those curves are obviously incomparable because of quite different country populations. We can convert to per-million by applying arithmetic to the whole series (requires knowledge of the country population)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USA['new_cases']\n",
    "# USA['new_cases'] / 331000000             # USA population is about 331M, so this is per capita\n",
    "# USA['new_cases'] / 331000000 * 1000000   # per capita is too small, so scale up to per million"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as the above plot, except scaling per million (USA~331M, etc)\n",
    "USAperM = USA['new_cases']/331\n",
    "plt.figure(figsize=(16,8))\n",
    "ax = plt.gca()\n",
    "ax.plot(USAx, USAperM)\n",
    "ax.plot(ITA['date'], ITA['new_cases']/60) # again, all this can be done inline\n",
    "ax.plot(SWE['date'], SWE['new_cases']/10)\n",
    "ax.plot(KOR['date'], KOR['new_cases']/51)\n",
    "ax.legend(['USA', 'Italy', 'Sweden', 'South Korea'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's smooth out those weekly cycles with a 7-day average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USAperMrollWeek = USAperM.rolling(window=7) # this tracks statistics on a rolling window of 7 rows (days)\n",
    "USAperMweekAvg  = USAperMrollWeek.mean()    # this grabs the mean() (average), as opposed to min/max/med, etc.\n",
    "plt.figure(figsize=(16,8))\n",
    "ax=plt.gca()\n",
    "ax.plot(USAx, USAperMweekAvg)\n",
    "ax.plot(ITA['date'], ITA['new_cases'].rolling(window=7).mean()/60)  # These inlines are starting to get \n",
    "ax.plot(SWE['date'], SWE['new_cases'].rolling(window=7).mean()/10)  #    unmanageably complex\n",
    "ax.plot(KOR['date'], KOR['new_cases'].rolling(window=7).mean()/51)\n",
    "ax.legend(['USA', 'Italy', 'Sweden', 'South Korea'])\n",
    "\n",
    "\n",
    "# Try these out...\n",
    "#plt.axvline('2020-12-09', c='r', ls=':')\n",
    "#plt.text(x='2020-12-15', y=100, s=\"Last year's class\", c='r')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "Modify this notebook to make the following changes:\n",
    "\n",
    "1. Remove S. Korea (they did so well, they can barely be seen on the graph!)\n",
    "1. Add **TWO** countries of your choosing that make this graph more interesting (note you'll have to google the population of those two new countries)\n",
    "1. Change the graph to be **deaths per million** (so far all the graphs have been  **cases per million**) \n",
    "\n",
    "Here is a table of abbreviations that you might find helpful (but you don't have to pick from these)\n",
    "\n",
    "|iso_code  | location |iso_code  | location |\n",
    "|:---------|:---------|:---------|:---------|\n",
    "|AUS       | Australia|IRN       | Iran     |\n",
    "|BRA       | Brazil   |ISR       | Israel   |\n",
    "|CAN       | Canada   |MEX       | Mexico   |\n",
    "|ESP       | Spain    |NZL       | New Zealand |\n",
    "|FRA       | France   |RUS       | Russia   |\n",
    "|GBR       | United Kingdom |ZAF | South Africa |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# World Aggregation\n",
    "We can add all the countries together. `groupby(['date']).sum()` says we want a new dataframe, with a row for every unique date, and all the other columns are added up per date.\n",
    "\n",
    "(For some situations, it might make more sense to `groupby(['date']).mean()`)\n",
    "\n",
    "Note how `world.head()` prints the date in bold; that's because it is now the 'index', not a regular column. Repeat the cell with `.reset_index()` active, and you will see that date is set to a regular column again (and the index is just a running counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "world = df.groupby(['date']).sum()    #.reset_index()\n",
    "world.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the same kinds of plotting that happened above can be done with this world dataframe. If `.reset_index()` is used, then the x for the plots can be `world['date']` as before; if `.reset_index()` is *not* used, then the x for plotting must be `world.index`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateX   = world.index         # or world['date'] if .reset_index() is used\n",
    "casesY  = world['new_cases']  # rolling average gets applied on these two lines\n",
    "deathsY = world['new_deaths']\n",
    "\n",
    "plt.figure()\n",
    "ax = plt.gca()\n",
    "ax.plot(dateX, casesY)\n",
    "\n",
    "# This is how you have a 2nd y axis on the right; two Axes share the same (twin) X axis\n",
    "ax2 = ax.twinx()  \n",
    "ax2.plot(dateX, deathsY)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "Improve the graph above in the following ways:\n",
    "1. Initialize the figure to have a larger figsize with an attractive aspect ratio.\n",
    "1. Use a 7-day rolling average instead of the raw numbers\n",
    "1. Use set_ylabel() to describe the left and right axes, and use set_title() to title the whole chart\n",
    "1. Use different colors for the cases/deaths graphs\n",
    "1. Use plt.legend()\n",
    "1. Use the plt.axvline() example above to annotate a few significant dates, such as the start of vaccination, the discovery of the Delta/Omicron variants, etc.\n",
    "1. Use plt.text() or plt.annotate() (example in the MatplotlibIntro notebook) to annotate 'First Wave', 'Second Wave', etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Optional extra: Shifted Dates\n",
    "\n",
    "Most complicated, we can see that these curves would be more comparable if they were date-shifted, to reflect the different times when the pandemic hit different countries. A common technique is to line them all up based on when they had a certain common minimum number of cases, say 10. We will filter on a condition again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I could just type 100 in every line below, but this way if I want to experiment with a different value\n",
    "# I can edit just 1 line, instead of having to edit a line for every country (especially as countries are added)\n",
    "min_cases = 100\n",
    "USAsh = USA[ USA['total_cases'] >= min_cases ]  # 'sh' for shift\n",
    "ITAsh = ITA[ ITA['total_cases'] >= min_cases ]\n",
    "SWEsh = SWE[ SWE['total_cases'] >= min_cases ]\n",
    "KORsh = KOR[ KOR['total_cases'] >= min_cases ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USAsh['date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USAsh['date'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITAsh['date'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USAsh['date'].min() - ITAsh['date'].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see above that Italy reached 100 cases on Feb 24, 8 days before the US on Mar 3. (And that date objects can be subtracted!)\n",
    "\n",
    "Here are all the dates where these countries reached 100 cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USAt0 = USAsh['date'].min()\n",
    "ITAt0 = ITAsh['date'].min()\n",
    "SWEt0 = SWEsh['date'].min()\n",
    "KORt0 = KORsh['date'].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like we were able to simply multiply and divide the entire 'new_cases' Series by constant numbers, we can subtract the start date from the date Series, yielding number of days since 100 cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USAsh['date'] - USAt0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "USAshX = USAsh['date'] - USAt0\n",
    "USAshY = USAsh['new_cases'].rolling(window=7).mean()/331\n",
    "plt.figure(figsize=(16,8))\n",
    "ax = plt.gca()\n",
    "ax.plot(USAshX, USAshY)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that plot goes from 0 to 2.5e16. Even though the description of `USAsh['date'] - USAt0` above says 'days', matplotlib is interpreting it as milliseconds. We can fix this by forcing conversion to days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USAshX = (USAsh['date'] - USAt0).astype('timedelta64[D]')   # 'D' is for Days\n",
    "USAshX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "ax = plt.gca()\n",
    "ax.plot(USAshX, USAshY)\n",
    "ax.plot((ITAsh['date']-ITAt0).astype('timedelta64[D]'), ITAsh['new_cases'].rolling(window=7).mean()/60)  \n",
    "ax.plot((SWEsh['date']-SWEt0).astype('timedelta64[D]'), SWEsh['new_cases'].rolling(window=7).mean()/10) \n",
    "ax.plot((KORsh['date']-KORt0).astype('timedelta64[D]'), KORsh['new_cases'].rolling(window=7).mean()/51)\n",
    "ax.legend(['USA', 'Italy', 'Sweden', 'South Korea'])\n",
    "ax.set_xlim( (0, 300) )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
